import torch
import syft

from typing import List, Tuple

from syft.exceptions import SendNotPermittedError
from syft.generic.abstract.tensor import AbstractTensor
from syft.generic.frameworks.hook import hook_args
from syft.generic.frameworks.overload import overloaded
from syft.workers.abstract import AbstractWorker
from syft.frameworks.torch.tensors.interpreters.additive_shared import AdditiveSharingTensor
from syft.generic.pointers.multi_pointer import MultiPointerTensor


class PrivateTensor(AbstractTensor):
    def __init__(
        self,
        owner=None,
        id=None,
        field = None,
        dtype:str = None,
        tags: set = None,
        description: str = None,
        allowed_users: Tuple[str] = (),
        parents: Tuple[str] = (),
        command: str = None,
    ):
        """ Initialize a Private tensor, which manages permissions restricting get operations.

        Args:
            owner (BaseWorker, optional): A BaseWorker object to specify the worker on which
            the tensor is located.
            id (string or int, optional): An optional string or integer id of the PrivateTensor.
            tags (set, optional): A set of tags to label this tensor.
            description (string, optional): A brief description about this tensor.
            allowed_users (Tuple, optional): User credentials.
            parents (tuple, optional): If it was generated by other tensors, it'll be
                                    referenced here.
            command (string, optional): If it was generated by some operation, it'll be
                                    registered here.
        """
        super().__init__(tags=tags, description=description)
        self.field = field
        self.dtype = dtype
        self.owner = owner
        self.id = id if id else syft.ID_PROVIDER.pop()
        self.child = None
        self.allowed_users = allowed_users
        self.parents = parents
        self.command = command

    def get_class_attributes(self):
        """ Specify all the attributes need to build a wrapper correctly when returning
        a response.
        """
        return {"allowed_users": self.allowed_users,"dtype": self.dtype,
}

    def allow(self, user) -> bool:
        """ Overwrite native's allowed to verify if a specific user is allowed to get this tensor.

        Args:
            user (object): user to be verified.

        Returns:
            bool : A boolean value (True if the user is allowed and false if it isn't).
        """
        return user in self.allowed_users

    def _before_send(self, *location, user: object = None, **kwargs):
        if not self.allow(user):
            raise SendNotPermittedError()

    def register_credentials(self, users: List[str]) -> "PrivateTensor":
        """ Register a new user credential(s) into the list of allowed users to get this tensor.

            Args:
                users (list): Credential(s) to be registered.
        """
        if not hasattr(self, "allowed_users"):
            self.allowed_users = ()

        self.allowed_users = self.allowed_users + tuple(users)

        return self

    def float_precision(self):
        """ Forward float_precision method to next child on tensor stack. """
        return self.child.float_precision()

    @overloaded.method
    def add(self, _self, other):
        """Add two Private tensors together.
        """
        if isinstance(other, (int, float)):
            scaled_int = int(other * self.base ** self.precision_fractional)
            return getattr(_self, "add")(scaled_int)

        if isinstance(_self, AdditiveSharingTensor) and isinstance(other, torch.Tensor):
            # If we try to add a PT>(wrap)>AST and a PT>torch.tensor,
            # we want to perform AST + torch.tensor
            other = other.wrap()
        elif isinstance(other, AdditiveSharingTensor) and isinstance(_self, torch.Tensor):
            # If we try to add a PT>torch.tensor and a PT>(wrap)>AST,
            # we swap operators so that we do the same operation as above
            _self, other = other, _self.wrap()

        response = getattr(_self, "add")(other)

        return response

    __add__ = add
    __radd__ = add

    def add_(self, value_or_tensor, tensor=None):
        if tensor is None:
            result = self.add(value_or_tensor)
        else:
            result = self.add(value_or_tensor * tensor)

        self.child = result.child
        return self

    def __iadd__(self, other):
        """Add two Private tensors together.
        """
        self.child = self.add(other).child

        return self

    @overloaded.method
    def sub(self, _self, other):
        """Subtracts a Private tensor from another one.
        """
        if isinstance(other, (int, float)):
            scaled_int = int(other * self.base ** self.precision_fractional)
            return getattr(_self, "sub")(scaled_int)

        if isinstance(_self, AdditiveSharingTensor) and isinstance(other, torch.Tensor):
            # If we try to subtract a PT>(wrap)>AST and a PT>torch.tensor,
            # we want to perform AST - torch.tensor
            other = other.wrap()
        elif isinstance(other, AdditiveSharingTensor) and isinstance(_self, torch.Tensor):
            # If we try to subtract a PT>torch.tensor and a PT>(wrap)>AST,
            # we swap operators so that we do the same operation as above
            _self, other = -other, -_self.wrap()

        response = getattr(_self, "sub")(other)

        return response

    __sub__ = sub

    def __rsub__(self, other):
        return (self - other) * -1

    def sub_(self, value_or_tensor, tensor=None):
        if tensor is None:
            result = self.sub(value_or_tensor)
        else:
            result = self.sub(value_or_tensor * tensor)

        self.child = result.child
        return self

    def __isub__(self, other):
        self.child = self.sub(other).child

        return self

    @overloaded.method
    def t(self, _self, *args, **kwargs):
        """Transpose a tensor. Hooked is handled by the decorator"""
        response = getattr(_self, "t")(*args, **kwargs)

        return response
#














    @staticmethod
    @overloaded.module
    def torch(module):
        def add(self, other):
            return self.__add__(other)

        module.add = add

        def sub(self, other):
            return self.__sub__(other)

        module.sub = sub

        def mul(self, other):
            return self.__mul__(other)

        module.mul = mul

        def div(self, other):
            return self.__truediv__(other)

        module.div = div

        def matmul(self, other):
            return self.matmul(other)

        module.matmul = matmul
        module.mm = matmul

        def addmm(bias, input_tensor, weight):
            matmul = input_tensor.matmul(weight)
            result = bias.add(matmul)
            return result

        module.addmm = addmm

        def dot(self, other):
            return self.__mul__(other).sum()

        module.dot = dot

        # You can also overload functions in submodules!
        @overloaded.module
        def nn(module):
            """
            The syntax is the same, so @overloaded.module handles recursion
            Note that we don't need to add the @staticmethod decorator
            """

            @overloaded.module
            def functional(module):
                def linear(*args):
                    """
                    Un-hook the function to have its detailed behaviour
                    """
                    return torch.nn.functional.native_linear(*args)

                module.linear = linear

            module.functional = functional

        # Modules should be registered just like functions
        module.nn = nn


    @classmethod
    def handle_func_command(cls, command):
        """
        Receive an instruction for a function to be applied on a Private Tensor,
        Perform some specific action (like logging) which depends of the
        instruction content, replace in the args all the PTensors with
        their child attribute, forward the command instruction to the
        handle_function_command of the type of the child attributes, get the
        response and replace a Private on top of all tensors found in
        the response.
        :param command: instruction of a function command: (command name,
        <no self>, arguments[, kwargs_])
        :return: the response of the function command
        """
        cmd, _, args_, kwargs_ = command

        tensor = args_[0] if not isinstance(args_[0], (tuple, list)) else args_[0][0]

        # Check that the function has not been overwritten
        try:
            # Try to get recursively the attributes in cmd = "<attr1>.<attr2>.<attr3>..."
            cmd = cls.rgetattr(cls, cmd)
            return cmd(*args_, **kwargs_)
        except AttributeError:
            pass

        # Replace all PrivateTensor with their child attribute
        new_args, new_kwargs, new_type = hook_args.unwrap_args_from_function(cmd, args_, kwargs_)

        # build the new command
        new_command = (cmd, None, new_args, new_kwargs)

        # Send it to the appropriate class and get the response
        response = new_type.handle_func_command(new_command)

        # Put back PrivateTensor on the tensors found in the response
        response = hook_args.hook_response(
            cmd, response, wrap_type=cls, wrap_args=tensor.get_class_attributes()
        )

        return response

    def share(self, *owners, protocol=None, field=None, dtype=None, crypto_provider=None):
        """
        Forward the .share() command to the child tensor, and reconstruct a new
        PrivateTensor since the command is not inplace and should return
        a new chain
        Args:
            *owners: the owners of the shares of the resulting AdditiveSharingTensor
            protocol: the crypto protocol used to perform the computations ('snn' or 'fss')
            dtype: the dtype in which the share values live
            crypto_provider: the worker used to provide the crypto primitives used
                to perform some computations on AdditiveSharingTensors
        Returns:
            A PrivateTensor whose child has been shared
        """
        if dtype is None:
            dtype = self.dtype
        else:
            assert (
                dtype == self.dtype
            ), "When sharing a PrivateTensor, the dtype of the resulting AdditiveSharingTensor \
                must be the same as the one of the original tensor"

        tensor = PrivateTensor(owner=self.owner, **self.get_class_attributes())

        tensor.child = self.child.share(
            *owners, protocol=protocol, dtype=dtype, crypto_provider=crypto_provider, no_wrap=True
        )
        return tensor

    def share_(self, *args, **kwargs):
        """
        Performs an inplace call to share. The PrivateTensor returned is therefore the same,
        contrary to the classic share version
        """
        self.child = self.child.share_(*args, no_wrap=True, **kwargs)
        return self


    @staticmethod
    def simplify(worker: AbstractWorker, tensor: "PrivateTensor") -> tuple:
        """Takes the attributes of a PrivateTensor and saves them in a tuple.

        Args:
            tensor (PrivateTensor): a PrivateTensor.

        Returns:
            tuple: a tuple holding the unique attributes of the fixed private tensor.
        """

        chain = None
        if hasattr(tensor, "child"):
            chain = syft.serde.msgpack.serde._simplify(worker, tensor.child)

        return (
            syft.serde.msgpack.serde._simplify(worker, tensor.id),
            syft.serde.msgpack.serde._simplify(worker, tensor.field),
            tensor.dtype,
            syft.serde.msgpack.serde._simplify(worker, tensor.allowed_users),
            syft.serde.msgpack.serde._simplify(worker, tensor.tags),
            syft.serde.msgpack.serde._simplify(worker, tensor.description),
            chain,
        )

    @staticmethod
    def detail(worker: AbstractWorker, tensor_tuple: tuple) -> "PrivateTensor":
        """
        This function reconstructs a PrivateTensor given it's attributes in form of a tuple.
        Args:
            worker (AbstractWorker): the worker doing the deserialization
            tensor_tuple (tuple): a tuple holding the attributes of the PrivateTensor
        Returns:
            PrivateTensor: a PrivateTensor
        Examples:
            shared_tensor = detail(data)
        """

        (tensor_id,field,dtype, allowed_users, tags, description, chain) = tensor_tuple

        tensor = PrivateTensor(
            owner=worker,
            id=syft.serde.msgpack.serde._detail(worker, tensor_id),
            field=syft.serde.msgpack.serde._detail(worker, field),
            dtype=dtype,
            tags=syft.serde.msgpack.serde._detail(worker, tags),
            description=syft.serde.msgpack.serde._detail(worker, description),
            allowed_users=syft.serde.msgpack.serde._detail(worker, allowed_users),
        )

        if chain is not None:
            chain = syft.serde.msgpack.serde._detail(worker, chain)
            tensor.child = chain

        return tensor



### Register the tensor with hook_args.py ###
hook_args.default_register_tensor(PrivateTensor)
